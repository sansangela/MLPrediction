{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Params\n",
    "input_seq_length = 8\n",
    "output_seq_length = 12\n",
    "seq_length = input_seq_length + output_seq_length\n",
    "num_features = 2\n",
    "hidden_dim = 128\n",
    "embedding_dim = 64\n",
    "grad_clip = 10.0\n",
    "\n",
    "# Discretization\n",
    "neighborhood_size = 32\n",
    "# grid_ratio = 0.001\n",
    "grid_size = 0.5\n",
    "# spatial_pooling_size = 32\n",
    "# pooling_window_size = (8, 8)\n",
    "\n",
    "# Training Params\n",
    "num_epochs = 5\n",
    "batch_size = 5\n",
    "learning_rate = 0.005\n",
    "decay_rate = 0.95\n",
    "dropout_keep_prob = 0.8\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 23:09:17.541059: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-26 23:09:17.895146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-26 23:09:17.895203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-26 23:09:17.952118: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-26 23:09:18.051197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 23:09:18.969361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load preprocessed data\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoader\n",
    "\n",
    "file_path = \"/root/Projects/MLPrediction/data/eth/hotel/pixel_pos_interpolatae.csv\"\n",
    "dataset_name = \"eth_hotel\"\n",
    "file_path_processed = \"/root/Projects/MLPrediction/data/preprocessed/\"\n",
    "\n",
    "dataloader = DataLoader(file_path, dataset_name, file_path_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 23:09:22.353547: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 23:09:22.589334: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-26 23:09:22.589375: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, ReLU, LSTMCell, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import time\n",
    "\n",
    "# Check TensorFlow version\n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version:\", tf_version)\n",
    "\n",
    "# Check if there is a GPU available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_occupancy_map(dense_representation, ped_id_to_index_map, num_peds_per_seq, ped_per_frame, is_animate=False):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        dense_representation (numpy.ndarray): _description_\n",
    "        ped_id_to_index_map (dict): _description_\n",
    "        num_peds_per_seq (_type_): _description_\n",
    "        ped_per_frame (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        occupancy_grid (lsit(tf.Variable)): occupancy_grid[i] with shape (num_peds_per_seq, num_peds_per_seq, neighborhood_size**2)\n",
    "    \"\"\"\n",
    "    grid_size_half = grid_size / 2.0\n",
    "\n",
    "    if is_animate:\n",
    "        plot_animation(dense_representation, ped_id_to_index_map, [1,3])\n",
    "\n",
    "    occupancy_grid = []\n",
    "\n",
    "    for frame_id in range(seq_length):\n",
    "        occupancy_grid_frame = np.zeros((num_peds_per_seq, num_peds_per_seq, neighborhood_size, neighborhood_size))\n",
    "        \n",
    "        ped_indices = [ped_id_to_index_map[ped_id] for ped_id in ped_per_frame[frame_id]]\n",
    "\n",
    "        for ped_i, ped_j in itertools.permutations(ped_indices, 2):\n",
    "            ped_i_y, ped_i_x = dense_representation[frame_id, ped_i]\n",
    "            ped_j_y, ped_j_x = dense_representation[frame_id, ped_j]\n",
    "            ped_i_neighbor_y_low, ped_i_neighbor_x_low = ped_i_y - grid_size_half, ped_i_x - grid_size_half\n",
    "            ped_i_neighbor_y_high, ped_i_neighbor_x_high = ped_i_y + grid_size_half - 1, ped_i_x + grid_size_half - 1\n",
    "\n",
    "            if ped_j_y >= ped_i_neighbor_y_low and ped_j_y <= ped_i_neighbor_y_high and ped_j_x >= ped_i_neighbor_x_low and ped_j_x <= ped_i_neighbor_x_high:\n",
    "                # ped_j in ped_i neighborhood\n",
    "                cell_y = int(np.floor((ped_j_y - ped_i_neighbor_y_low) / grid_size * neighborhood_size))\n",
    "                cell_x = int(np.floor((ped_j_x - ped_i_neighbor_x_low) / grid_size * neighborhood_size))\n",
    "                occupancy_grid_frame[ped_i, ped_j, cell_y, cell_x] = 1\n",
    "\n",
    "        occupancy_grid.append(tf.Variable(tf.reshape(occupancy_grid_frame, (num_peds_per_seq, num_peds_per_seq, -1))))\n",
    "\n",
    "    return occupancy_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialLSTM(Model):\n",
    "    def __init__(self, embedding_dim=64, hidden_dim=128, pool_size=(8,8), neighbors=32, num_features=2, out_features=5, dropout=0.5):\n",
    "        super(SocialLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_embedding_layer = Dense(embedding_dim)\n",
    "        self.social_embedding_layer = Dense(embedding_dim)  # TODO\n",
    "        self.lstm = LSTMCell(hidden_dim)\n",
    "        self.output_embedding_layer = Dense(out_features)\n",
    "        self.dropout_layer = Dropout(dropout)\n",
    "\n",
    "        self.relu = ReLU()\n",
    "\n",
    "\n",
    "    def call(self, input, lstm_hidden, lstm_cell, occupancy_grid):\n",
    "        # occupancy_grid: (seq_length, num_peds_per_seq, num_peds_per_seq, neighborhood_size*neighborhood_size)\n",
    "        out_split = tf.split(tf.zeros(input.shape), num_or_size_splits=input.shape[0], axis=0)\n",
    "                    \n",
    "        for frame_id in range(input.shape[0]):\n",
    "            # Input embedding\n",
    "            input_t = input[frame_id]\n",
    "            input_embed_out = self.input_embedding_layer(input_t)\n",
    "            input_embed_out = self.relu(input_embed_out)\n",
    "            # input_embed_out = self.dropout_layer(input_embed_out)\n",
    "\n",
    "            # Social embedding\n",
    "            occupancy_grid_frame = occupancy_grid[frame_id]\n",
    "            occupancy_grid_frame_float32 = tf.cast(occupancy_grid_frame, dtype=tf.float32)  # cast from float64 to float32\n",
    "\n",
    "            result = tf.einsum('ijk,jx->ikx', occupancy_grid_frame_float32, lstm_hidden)    # (num_peds_per_seq, num_peds_per_seq, neighborhood_size**2), (num_peds_per_seq, hidden_dim) -> (num_peds_per_seq, neighborhood_size**2, hidden_dim)\n",
    "            result = tf.reshape(result, (result.shape[0], -1))\n",
    "\n",
    "            social_embed_out = self.social_embedding_layer(result)\n",
    "            social_embed_out = self.relu(social_embed_out)\n",
    "            # social_embed_out = self.dropout_layer(social_embed_out)\n",
    "\n",
    "            # Concat input embedding and social embedding outputs\n",
    "            embed_out = tf.concat([input_embed_out, social_embed_out], axis=1)\n",
    "\n",
    "            # LSTM\n",
    "            lstm_out, new_state = self.lstm(embed_out, (lstm_hidden, lstm_cell))\n",
    "            lstm_hidden, lstm_cell = new_state\n",
    "            \n",
    "            # Output embedding\n",
    "            out_t = self.output_embedding_layer(lstm_out)\n",
    "\n",
    "            out_t = tf.expand_dims(out_t, axis=0)\n",
    "\n",
    "            out_split[frame_id] = out_t\n",
    "        \n",
    "        out = tf.concat(out_split, axis=0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred, ped_id_to_index_map):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) loss between predicted and true positions.\n",
    "\n",
    "    Args:\n",
    "        y_true (list): True pedestrian positions of shape (num_sequences, np.array(num_pedestrians, 3)).\n",
    "        y_pred (tf.Tensor): Predicted pedestrian positions of shape (num_sequences, num_pedestrians, 2).\n",
    "        ped_id_to_index_map (dict): Dictionary mapping pedestrian IDs to their indices.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Mean Squared Error loss.\n",
    "    \"\"\"\n",
    "    num_seq, num_ped = len(y_true), len(ped_id_to_index_map)\n",
    "    y_true_dense_representation = np.zeros((num_seq, num_ped, 2))\n",
    "    for sequence_idx in range(len(y_true)):\n",
    "        indices = [ped_id_to_index_map[x] for x in y_true[sequence_idx][:, 0] if x in ped_id_to_index_map.keys()]\n",
    "        if not indices:\n",
    "            continue\n",
    "        y_true_dense_representation[sequence_idx, indices, :] = y_true[sequence_idx][:, 1:3]\n",
    "    \n",
    "    return -tf.math.reduce_mean(y_true_dense_representation-y_pred)\n",
    "\n",
    "def gaussian_loss(y_true, y_out, ped_id_to_index_map, input_seq_length = 8, output_seq_length = 12):\n",
    "    # y_out: shape (seq_length, num_ped, 5)\n",
    "    num_seq, num_ped = len(y_true), len(ped_id_to_index_map)\n",
    "    y_true_dense_representation = np.zeros((num_seq, num_ped, 2))\n",
    "    for sequence_idx in range(len(y_true)):\n",
    "        indices = [ped_id_to_index_map[x] for x in y_true[sequence_idx][:, 0] if x in ped_id_to_index_map.keys()]\n",
    "        if not indices:\n",
    "            continue\n",
    "        y_true_dense_representation[sequence_idx, indices, :] = y_true[sequence_idx][:, 1:3]\n",
    "    \n",
    "    mu_y, mu_x, sigma_y, sigma_x, rho = y_out[:,:,0], y_out[:,:,1], y_out[:,:,2], y_out[:,:,3], y_out[:,:,4]\n",
    "    \n",
    "    y_offset = y_true_dense_representation[:,:,0] - mu_y\n",
    "    x_offset = y_true_dense_representation[:,:,1] - mu_x\n",
    "\n",
    "    z = (x_offset/sigma_x)**2 + (y_offset/sigma_y)**2 - 2.0*rho*x_offset*y_offset/(sigma_x*sigma_y)\n",
    "    constant = - 1.0 / (2.0 * (1 - rho**2))\n",
    "\n",
    "    # epsilon = 1e-20\n",
    "    # result = tf.math.exp(constant*z) / (2*tf.constant(np.pi)*sigma_x*sigma_y*tf.math.sqrt(1-rho**2))\n",
    "    # result = -tf.math.log(tf.clip_by_value(result, clip_value_min=epsilon, clip_value_max=tf.float32.max))\n",
    "\n",
    "    # Sum over all pedestrians\n",
    "    result = constant * z - 0.5 * tf.math.log(2 * tf.constant(np.pi) * sigma_x**2 * sigma_y**2)\n",
    "    result = tf.reduce_sum(result, axis=1) / num_ped\n",
    "\n",
    "    loss = tf.reduce_sum(result[input_seq_length:input_seq_length+output_seq_length]) / output_seq_length\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU...\n"
     ]
    }
   ],
   "source": [
    "from utils import mse_loss\n",
    "\n",
    "device = '/CPU:0'\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device = '/GPU:0'\n",
    "    print(\"Using GPU...\")\n",
    "\n",
    "dataloader.reset_frame_ptr()\n",
    "with tf.device(device):\n",
    "\n",
    "    # Declare model and optimizer\n",
    "    model = SocialLSTM()\n",
    "    lr = tf.Variable(learning_rate, trainable=False)\n",
    "    optimizer = RMSprop(lr, clipvalue=grad_clip)\n",
    "\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0      \n",
    "\n",
    "        # For each batch  \n",
    "        for batch in dataloader.generate_batches():\n",
    "            inputs, targets, batch_ped_indices = batch\n",
    "            if not inputs or not targets:\n",
    "                # Traverse to the end\n",
    "                break\n",
    "\n",
    "            # ped_per_frame = dataloader.ped_indices\n",
    "            batch_loss = 0.0\n",
    "\n",
    "            # For each sequence\n",
    "            for sequence_idx in range(batch_size):\n",
    "                input, target, ped_per_frame = inputs[sequence_idx], targets[sequence_idx], batch_ped_indices[sequence_idx]\n",
    "\n",
    "                dense_representation, ped_id_to_index_map = dataloader.convert_to_dense_representation(input) \n",
    "                dense_representation_tf = tf.Variable(tf.convert_to_tensor(dense_representation))\n",
    "                num_peds_per_frame = len(ped_id_to_index_map)\n",
    "\n",
    "                # Create occupancy grid\n",
    "                occupancy_grid = get_occupancy_map(dense_representation, ped_id_to_index_map, num_peds_per_frame, ped_per_frame)\n",
    "                \n",
    "                # Initialize LSTM params\n",
    "                lstm_hidden = tf.Variable(tf.zeros((num_peds_per_frame, hidden_dim)))\n",
    "                lstm_cell = tf.Variable(tf.zeros((num_peds_per_frame, hidden_dim)))\n",
    "\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    out = model(dense_representation_tf, lstm_hidden, lstm_cell, occupancy_grid)\n",
    "                    loss = gaussian_loss(target, out, ped_id_to_index_map)\n",
    "                    batch_loss += loss\n",
    "\n",
    "\n",
    "                # Compute gradients\n",
    "                grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "                # print(grads)\n",
    "\n",
    "                # Clip gradients by norm\n",
    "                grads, _ = tf.clip_by_global_norm(grads, grad_clip)\n",
    "\n",
    "                \n",
    "                # Update parameters\n",
    "                trainable_variables = model.trainable_variables\n",
    "                optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "            batch_loss /= batch_size\n",
    "            epoch_loss += batch_loss\n",
    "            \n",
    "        print('(epoch {}/{}), train_loss = {:.3f}'.format(\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
